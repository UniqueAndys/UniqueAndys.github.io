<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Hadoop实验--WordCount · Andy's blog</title><meta name="description" content="Hadoop实验--WordCount - Andy Song"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/u/2532176501" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://github.com/uniqueandys" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">Hadoop实验--WordCount</h1><div class="post-time">Oct 24, 2016</div><div class="post-content"><p>今天给大家带来Hadoop下的第一个Hello World程序。<br><a id="more"></a></p>
<h1 id="1-选用文本"><a href="#1-选用文本" class="headerlink" title="1. 选用文本"></a>1. 选用文本</h1><p>实验所选用的数据选用的是《冰与火之歌》(A Song of Ice and Fire)的英文txt文件，从下面这个网站中下载：<a href="http://persischempaka.blogspot.com/2012/04/game-of-thronestxt.html" target="_blank" rel="external">http://persischempaka.blogspot.com/2012/04/game-of-thronestxt.html</a><br>一共有五个txt文件，分别是:</p>
<ul>
<li>001ssb.txt (A Game of Thrones，权力的游戏，1.6M)</li>
<li>002ssb.txt (A Clash of Kings，列王的纷争，1.8M)</li>
<li>003ssb.txt (A Storm of Swords，冰与的风暴，2.4M)</li>
<li>004ssb.txt (A Feast for Crows，群鸦的盛宴，1.7M)</li>
<li>005ssb.txt (A Dance with Dragons，魔龙的狂舞，2.4M)</li>
</ul>
<h1 id="2-创建HDFS文件目录"><a href="#2-创建HDFS文件目录" class="headerlink" title="2. 创建HDFS文件目录"></a>2. 创建HDFS文件目录</h1><p>格式化文件系统并启动，因为伪分布式读取的是HDFS上的数据，要执行MapReduce任务，我们需要创建HDFS文件系统。使用HDFS文件操作命令：<code>hdfs dfs -mkdir －p /user/tyrion</code>，其中，<code>tyrion</code>是你当前Ubuntu的用户名。该命令的行为与UNIX下<code>mkdir -p</code>相似，这一路径上的父目录如果不存在，则创建该父目录。如果按如上命令，那么现在就默认在HDFS文件系统中的<code>/user/tyrion</code>目录下。接下来创建<code>input</code>文件夹用于存放输入数据，使用put命令将之前的txt文件拷入input文件夹中。类似，ls命令可以查看文件列表。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir input</span><br><span class="line">hdfs dfs -put /path_to_txt_file/*.txt input</span><br><span class="line">hdfs dfs -ls input</span><br></pre></td></tr></table></figure></p>
<p>在浏览器中输入<code>localhost:50070</code>可以查看集群基本信息，包括文件信息。<br><img src="/img/hadoop/file_list.png" alt=""></p>
<h1 id="3-使用IntelliJ编写Hadoop程序"><a href="#3-使用IntelliJ编写Hadoop程序" class="headerlink" title="3. 使用IntelliJ编写Hadoop程序"></a>3. 使用IntelliJ编写Hadoop程序</h1><ol>
<li>新建一个Project，一路默认。</li>
<li>添加依赖，依次打开<code>File - Project Structure - Modules - (右侧)+ - Library Type - Java</code>。选择hadoop目录下<code>share/hadoop</code>中的文件夹，本次实验WordCount只用到common和mapreduce文件夹，所以只添加这两个就行了。</li>
<li>在src下新建一个WordCount类，代码可参考官网Documentation中的Tutorial，在此不做赘述。</li>
<li>导出jar包。依次选择<code>File - Project Structure - Artifacts - + - JAR - From modules with dependencies...</code>，选择要Main Class，确定。接下来选择菜单<code>Build - Build Artifacts - Build</code>即可，生成的jar文件位于工程项目目录的<code>out/artifacts</code>下。</li>
</ol>
<h1 id="4-运行Hadoop程序"><a href="#4-运行Hadoop程序" class="headerlink" title="4. 运行Hadoop程序"></a>4. 运行Hadoop程序</h1><p>可以在hadoop下创建一个exercise文件夹，将刚才生成的jar包拷到其中，按如下命令运行:<br><code>hadoop jar ./exercise/wc.jar input output</code><br>使用命令<code>hdfs dfs -cat output/*</code>可以查看运行结果。<br>通过命令<code>hdfs dfs -get output ./test_out</code>可以将结果取回本地。</p>
<blockquote>
<p>运行Hadoop程序时，为了防止覆盖结果，程序指定的输出目录（如output）不能存在，否则会提示错误。</p>
</blockquote>
<p>也可以使用Intellij结合Maven本地运行和调试MapReduce程序，该方法最大的特点是不需要安装任何模式的Hadoop，只要在Maven配置文件中指定Hadoop依赖包名字和版本号，Maven就能自动搞定这些依赖。不过刚开始我还是偏向用IntelliJ以及命令行来，以后再来详细研究。</p>
<h1 id="5-在Web界面查看作业运行状态"><a href="#5-在Web界面查看作业运行状态" class="headerlink" title="5. 在Web界面查看作业运行状态"></a>5. 在Web界面查看作业运行状态</h1><p>要在Web端查看作业的执行情况，需要启动YARN，让YARN来负责资源管理与任务调度。新版Hadoop使用了新的MapReduce框架(YARN，Yet Another Resource Negotiator)。YARN是从MapReduce中分离出来，负责资源管理与任务调度。YARN运行于MapReduce之上，提供了高可用性、高扩展性。</p>
<ol>
<li>首先修改配置文件mapred-site.xml，原始只有.template文件，这边需要先进行重命名：<br><code>mv ./etc/hadoop/mapred-site.xml.template ./etc/hadoop/mapred-site.xml</code><blockquote>
<p>不启动YARN时需改回.template后缀，否则在该配置文件存在，而未开启YARN的情况下会一直尝试连接。</p>
</blockquote>
</li>
<li><p>修改mapred-site.xml以及yarn-site.xm文件。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="title">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="title">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="title">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>启动Hadoop <code>./sbin/start-dfs.sh</code><br>启动YARN <code>./sbin/start-yarn.sh</code><br>开启历史服务器，以便在Web中查看任务运行情况<code>./sbin/mr-jobhistory-daemon.sh start historyserver</code><br>开启后通过jps查看，可以看到多了NodeManager和ResourceManager两个后台进程。</p>
</li>
<li>浏览器打开<code>localhost:8088</code>可以查看任务运行情况，跑一下WordCount程序。程序执行过程中可以实时查看运行进度，跑完后点击History可以查看作业详细信息。<br><img src="/img/hadoop/web_1.png" alt=""><br><img src="/img/hadoop/web_2.png" alt=""></li>
<li>关闭脚本：<br><code>./sbin/mr-jobhistory-daemon.sh stop historyserver</code><br><code>./sbin/stop-yarn.sh</code><br><code>./sbin/stop-dfs.sh</code><blockquote>
<p>如果只启动YARN，那么只可以在Web查看任务运行状态，而不能查看历史信息。</p>
</blockquote>
</li>
</ol>
</div></article></div></section><footer><div class="paginator"><a href="/2016/11/02/wuliQQ/" class="prev">上一篇</a><a href="/2016/10/21/Hadoop-install/" class="next">下一篇</a></div><div class="copyright"><p>© 2015 - 2016 <a href="http://songbinbin.me">Andy Song</a>, unless otherwise noted.</p></div></footer><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "//hm.baidu.com/hm.js?a36e15d9e2adec9a21fcdd9f686b1ed2";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script src="https://cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>