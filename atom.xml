<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Andy&#39;s blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://songbinbin.me/"/>
  <updated>2017-02-28T12:43:43.112Z</updated>
  <id>http://songbinbin.me/</id>
  
  <author>
    <name>Andy Song</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习阅读笔记（三）</title>
    <link href="http://songbinbin.me/2017/02/28/Support-Vector-Machine-Reading-Notes/"/>
    <id>http://songbinbin.me/2017/02/28/Support-Vector-Machine-Reading-Notes/</id>
    <published>2017-02-28T12:29:31.000Z</published>
    <updated>2017-02-28T12:43:43.112Z</updated>
    
    <content type="html">&lt;p&gt;支持向量机阅读笔记，主要参考李航博士《统计学习方法》公式化描述、周志华老师《机器学习》形象讲解。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;线性可分支持向量机(硬间隔最大化)：求解能够正确划分训练数据集并且&lt;strong&gt;几何间隔&lt;/strong&gt;最大的分离超平面。可以表示为凸二次规划问题，其原始最优化问题为：&lt;br&gt;$$ \begin{align}&lt;br&gt;&amp;amp; \min_{w,b}\quad \frac{1}{2} \Vert w\Vert ^2 \\&lt;br&gt;&amp;amp; s.t.\quad y_i (w\cdot x_i + b)-1\ge 0,\quad i=1,2,\dots,N&lt;br&gt;\end{align} $$&lt;br&gt;二次规划的对偶问题为：&lt;br&gt;$$ \begin{align}&lt;br&gt;\min_\alpha\quad &amp;amp; \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j y_i y_j(x_i\cdot x_j) - \sum_{i=1}^N \alpha_i \\&lt;br&gt;s.t.\quad &amp;amp; \sum_{i=1}^N \alpha_i y_i = 0 \\&lt;br&gt;&amp;amp; \alpha_i \ge 0,\quad i=1,2,\dots, N&lt;br&gt;\end{align} $$&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;线性不可分意味着某些样本点 $(x_i, y_i)$ 不能满足函数间隔大于等于1的约束条件。为了解决这个问题，可以对每个样本点 $(x_i, y_i)$ 引入一个松弛变量 $\xi_i \ge 0$ ，使函数间隔加上松弛变量大于等于1。同时，对每个松弛变量 $\xi_i$ ，支付一个代价 $\xi_i$。线性不可分的线性支持向量机的学习问题变成如下凸二次规划问题（原始问题）：&lt;br&gt;$$ \begin{align}&lt;br&gt;\min_{w,b,\xi}\quad &amp;amp;  \frac{1}{2} \Vert w\Vert ^2 + C\sum_{i=1}^N \xi_i \\&lt;br&gt;s.t.\quad &amp;amp;  y_i (w\cdot x_i + b) + \xi_i \ge 1,\quad i=1,2,\dots,N \\&lt;br&gt;&amp;amp; \xi_i \ge 0,\quad i=1,2,\dots,N&lt;br&gt;\end{align} $$&lt;br&gt;其对偶形式为：&lt;br&gt;$$ \begin{align}&lt;br&gt;\min_\alpha\quad &amp;amp; \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j y_i y_j(x_i\cdot x_j) - \sum_{i=1}^N \alpha_i \\&lt;br&gt;s.t.\quad &amp;amp; \sum_{i=1}^N \alpha_i y_i = 0 \\&lt;br&gt;&amp;amp; 0 \le \alpha_i \le C,\quad i=1,2,\dots, N&lt;br&gt;\end{align} $$&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;核技巧可以用线性分类方法求解非线性分类问题，分为两步：首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;（&lt;strong&gt;核函数&lt;/strong&gt;）设 $\mathcal{X}$ 是输入空间（欧氏空间 $\mathbf{R}^n$ 的子集或离散集合），又设 $\mathcal{H}$ 为特征空间（希尔伯特空间），如果存在一个从 $\mathcal{X}$ 到 $\mathcal{H}$ 的映射 $$\phi(x):\mathcal{X} \to \mathcal{H}$$ 使得对所有的 $x,z \in \mathcal{X}$，函数 $K(x,z)$ 满足条件 $$K(x,z) = \phi(x) \cdot \phi(z)$$  则称 $K(x,z)$ 为核函数，$\phi(x)$ 为映射函数，式中 $\phi(x) \cdot \phi(z)$ 为 $\phi(x)$ 和 $\phi(z)$ 的内积。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;核技巧的想法是，在学习与预测中只定义核函数 $K(x,z)$，而不显示地定义映射函数 $\phi$。对于给定的核 $K(x,z)$，特征空间 $\mathcal{H}$ 和映射函数 $\phi$ 的取法并不唯一，可以取不同的特征空间，即便在同一特征空间里也可以取不同的映射。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;常用核函数&lt;br&gt;$$ \begin{align}&lt;br&gt;线性核\quad &amp;amp; k(x_i, x_j) = x_i^T x_j \\&lt;br&gt;多项式核\quad &amp;amp; k(x_i, x_j) = (x_i^T x_j)^d \\&lt;br&gt;高斯核（RBF核）\quad &amp;amp; k(x_i^T x_j) = exp(-\frac{\Vert x_i - x_j\Vert^2}{2\sigma^2}) \\&lt;br&gt;拉普拉斯核\quad &amp;amp; k(x_i^T x_j) = exp(-\frac{\Vert x_i - x_j\Vert}{\sigma}) \\&lt;br&gt;Sigmoid核\quad &amp;amp; k(x_i^T x_j) = tanh(\beta x_i^T x_j + \theta) \\&lt;br&gt;字符串核函数\quad &amp;amp; 定义在字符串集合上的核函数&lt;br&gt;\end{align} $$&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;SMO算法是支持向量机学习的一种快速算法，其特点是不断地将原二次规划问题分解为只有两个变量的二次规划问题，并对子问题进行解析求解，直到所有变量满足KKT条件为止。这样通过启发式的方式得到原二次规划问题的最优解。&lt;/li&gt;
&lt;/ul&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;支持向量机阅读笔记，主要参考李航博士《统计学习方法》公式化描述、周志华老师《机器学习》形象讲解。&lt;br&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://songbinbin.me/categories/Machine-Learning/"/>
    
    
      <category term="Reading Notes" scheme="http://songbinbin.me/tags/Reading-Notes/"/>
    
      <category term="Support Vector Machine" scheme="http://songbinbin.me/tags/Support-Vector-Machine/"/>
    
  </entry>
  
  <entry>
    <title>机器学习阅读笔记（二）</title>
    <link href="http://songbinbin.me/2017/02/22/Decision-Tree-Reading-Notes/"/>
    <id>http://songbinbin.me/2017/02/22/Decision-Tree-Reading-Notes/</id>
    <published>2017-02-22T11:19:36.000Z</published>
    <updated>2017-02-28T12:44:03.544Z</updated>
    
    <content type="html">&lt;p&gt;决策树阅读笔记，参考周志华老师的《机器学习》及李航博士的《统计学习方法》。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分类决策树模型是表示基于&lt;strong&gt;特征&lt;/strong&gt;对实例进行分类的树形结构。决策树可以转换成一个&lt;strong&gt;if-then&lt;/strong&gt;规则的集合，也可以是看作定义在特征空间划分上的&lt;strong&gt;类的条件概率分布&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;在信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量。设 $X$ 是一个取有限个值的离散随机变量，其概率分布为 $$ P(X=x_i) = p_i, i = 1,2, \dots,n $$&lt;br&gt;则随机变量 $X$ 的熵定义为 $$H(X) = - \sum_{i=1}^n p_i \log p_i$$ 熵越大，随机变量的不确定性就越大。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;设有随机变量 $(X,Y)$，其联合概率分布为 $$P(X=x_i,Y=y_i)=p_{ij},i=1,2,\dots,n;j=1,2,\dots,m$$ 条件熵 $H(Ya|X)$ 表示在已知随机变量 $X$ 的条件下随机变量 $Y$ 的不确定性。随机变量 $X$ 给定的条件下随机变量 $Y$ 的条件熵（conditional entropy） $H(Y|X)$，定义为 $X$ 给定条件下 $Y$ 的条件概率分布的熵对 $X$ 的数学期望 $$H(Y|X)=\sum_{i=1}^n p_i H(Y|X=x_i)$$ 这里，$p_i = P(X=x_i),i=1,2,\dots,n$&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵分别称为经验熵（empirical entropy）和经验条件熵（empirical conditional entropy）。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;特征 $A$ 对训练数据集 $D$ 的信息增益（information gain）表示得知特征 $X$ 的信息而使类 $Y$ 的信息的不确定性减少的程度。（偏向于选择取值较多的特征） $$g(D,A) = H(D) - H(D|A)$$&lt;/li&gt;
&lt;li&gt;特征 $A$ 对训练数据集 $D$ 的信息增益比（information gain ratio）定义为其信息增益 $g(D,A)$ 与训练数据集 $D$ 关于特征 $A$ 的值的熵 $H_A(D)$ 之比（对可取值数目较少的属性有所偏好），即 $$ g_R(D,A) = \frac{g(D,A)}{H_A(D)}$$&lt;/li&gt;
&lt;li&gt;基尼值反映了从数据集 $D$ 中随机抽取两个样本，其类别标记不一致的概率 $$Gini(D) = 1 - \sum_{k=1}^\mathcal{Y}p_k^2$$ 特征 $A$ 对训练数据集 $D$ 的基尼指数（Gini index）定义为 $$Gini_index(D,A) = \sum_{v=1}^V \frac{\vert D^v\vert}{\vert D\vert}Gini(D^v)$$&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;决策树剪枝的基本策略有&lt;strong&gt;预剪枝&lt;/strong&gt;（prepruning）和&lt;strong&gt;后剪枝&lt;/strong&gt;（postpruning）。&lt;br&gt;预剪枝是指在决策树生成过程中，对每个结点&lt;strong&gt;在划分前先进行估计&lt;/strong&gt;，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点。&lt;br&gt;后剪枝则是先从训练集生成一棵完整的决策树，然后&lt;strong&gt;自底向上地对非叶结点进行考察&lt;/strong&gt;，若将该叶结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;预剪枝与后剪枝对比&lt;br&gt;预剪枝基于&lt;strong&gt;贪心&lt;/strong&gt;本质使得决策树的很多分支都没有“展开”，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间开销，但带来了&lt;strong&gt;欠拟合&lt;/strong&gt;的风险。&lt;br&gt;后剪枝决策树通常比预剪枝决策树保留了更多的分支。一般情况下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树，但训练时间开销较大。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;统计学习方法中决策树的损失函数：设树 $T$ 的叶结点个数为 $|T|$，$t$ 是树 $T$ 的叶结点，该叶结点有 $N_t$ 个样本点，$H_t(T)$ 为叶结点 $t$ 上的经验熵，$\alpha \ge 0$ 为参数，则决策树学习的损失函数可定义为 $$C_\alpha (T) = \sum_{t=1}^{|T|}N_tH_t(T) + \alpha |T|$$&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;CART剪枝算法由两步组成：首先从生成算法产生的决策树 $T_0$ 底端开始不断剪枝，直到 $T_0$ 的根结点，形成一个子树序列 $\{ T_0, T_1, \dots, T_n\}$；然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;连续值处理，二分法，从小到大排序后依次取中位点划分。&lt;br&gt;缺失值处理，样本赋权，权重划分，让同一个样本以不同的概率划入到不同的子结点中去。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;多变量决策树&lt;/li&gt;
&lt;/ul&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;决策树阅读笔记，参考周志华老师的《机器学习》及李航博士的《统计学习方法》。&lt;br&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://songbinbin.me/categories/Machine-Learning/"/>
    
    
      <category term="Decision Tree" scheme="http://songbinbin.me/tags/Decision-Tree/"/>
    
      <category term="Reading Notes" scheme="http://songbinbin.me/tags/Reading-Notes/"/>
    
  </entry>
  
  <entry>
    <title>机器学习阅读笔记（一）</title>
    <link href="http://songbinbin.me/2017/02/12/Machine-Learning-Reading-Notes-Foudamentals/"/>
    <id>http://songbinbin.me/2017/02/12/Machine-Learning-Reading-Notes-Foudamentals/</id>
    <published>2017-02-12T03:21:53.000Z</published>
    <updated>2017-02-28T12:43:57.892Z</updated>
    
    <content type="html">&lt;p&gt;整理出周志华老师《机器学习》里的一些基本概念以加深自己对机器学习的理解。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;绪论&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;机器学习致力于研究如何通过计算的手段，利用经验来改善系统自身的性能。  &lt;/li&gt;
&lt;li&gt;“模型”泛指从数据中学得的结果，在计算机上从数据中产生“模型”的算法为学习算法。  &lt;/li&gt;
&lt;li&gt;尽管训练集通常只是&lt;strong&gt;样本空间&lt;/strong&gt;的一个很小的采样，我们仍希望它能很好地反映出样本空间的特性，否则很难期望在训练集上学得的模型能在整个样本空间上都工作得很好。  &lt;/li&gt;
&lt;li&gt;归纳是从特殊到一般的泛化过程，即从具体的事实归结出一般性规律。“从样例中学习”是一个归纳的过程，因此亦称“归纳学习”。  &lt;/li&gt;
&lt;li&gt;现实问题中我们常面临很大的假设空间，但学习的过程是基于有限样本训练集进行的，因此，可能有多个假设与训练集一致，即存在着一个与训练集一致的“假设集合”，我们称之为“版本空间”。&lt;/li&gt;
&lt;li&gt;机器学习算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”。任何一个有效的机器学习算法必有其归纳偏好，归纳偏好可看作学习算法自身在一个可能很庞大的假设空间中对假设进行选择的启发式或“价值观”。&lt;/li&gt;
&lt;li&gt;“奥卡姆剃刀”是一种常用的、自然科学研究中最基本的原则，即“若有多个假设与观察一致，则选最简单的那个”。&lt;/li&gt;
&lt;li&gt;大数据时代的三大关键技术：机器学习，云计算，众包。机器学习提供数据分析能力，云计算提供数据处理能力，众包提供数据标注能力。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;模型评估与选择&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;过拟合：把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质。欠拟合：对训练样本的一般性质尚未学好。过拟合是无法彻底避免的，我们所能做的只是“缓解”，或者说减小其风险。&lt;/li&gt;
&lt;li&gt;单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。&lt;/li&gt;
&lt;li&gt;为减小因样本划分不同而引入的差别，$k$折交叉验证通常要随机使用不同的划分重复$p$次，最终的评估结果是这$p$次$k$折交叉验证结果的均值。&lt;/li&gt;
&lt;li&gt;自助法在数据集较小、难以有效划分训练/测试集时很有用；此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。&lt;/li&gt;
&lt;li&gt;混淆矩阵，查准率，查全率，F1&lt;/li&gt;
&lt;li&gt;P-R曲线，ROC曲线都是先把预测结果排序，然后依次截断；不同的是横纵坐标的度量有所差别。&lt;/li&gt;
&lt;li&gt;为权衡不同类型错误所造成的不同损失，可为错误赋予“非均等代价”。&lt;/li&gt;
&lt;li&gt;泛化误差可分解为偏差、方差与噪声之和。&lt;br&gt;偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；&lt;br&gt;方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；&lt;br&gt;噪声则表达了在当前任务上任何学习算法所达到的期望泛化误差的下界，即刻画了学习问题本身的难度。&lt;br&gt;偏差-方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;线性模型&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;对数几率回归（logistic regression）：用线性回归模型的预测结果去逼近真实标记的对数几率。&lt;/li&gt;
&lt;li&gt;LDA的思想非常朴素：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离。&lt;/li&gt;
&lt;li&gt;纠错输出码ECOC是将编码的思想引入类别拆分，并尽可能在解码过程中具有容错性。&lt;/li&gt;
&lt;li&gt;解决类别不平衡问题的三类做法：欠采样（EasyEnsemble），过采样，阈值移动。&lt;/li&gt;
&lt;li&gt;多标记学习(multi-label learning)：为一个样本同时预测出多个类别标记。&lt;/li&gt;
&lt;/ol&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;整理出周志华老师《机器学习》里的一些基本概念以加深自己对机器学习的理解。&lt;br&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://songbinbin.me/categories/Machine-Learning/"/>
    
    
      <category term="Reading Notes" scheme="http://songbinbin.me/tags/Reading-Notes/"/>
    
  </entry>
  
  <entry>
    <title>深入浅出学统计阅读笔记</title>
    <link href="http://songbinbin.me/2017/02/11/The-cartoon-introduction-to-statistics/"/>
    <id>http://songbinbin.me/2017/02/11/The-cartoon-introduction-to-statistics/</id>
    <published>2017-02-11T03:03:48.000Z</published>
    <updated>2017-02-28T12:45:11.408Z</updated>
    
    <content type="html">&lt;p&gt;寒假看了一本《深入浅出统计学》，觉得讲得很好，整理出阅读笔记。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;引言：统计无处不在&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;人人都用统计学的&lt;strong&gt;真正原因&lt;/strong&gt;：统计帮助我们在&lt;strong&gt;信息有限的情况下&lt;/strong&gt;，做出&lt;strong&gt;充满信心的决策&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;统计学的&lt;strong&gt;基本问题&lt;/strong&gt;：我们如何通过样本，充满自信地描述整体？&lt;/li&gt;
&lt;li&gt;我们可以通过统计&lt;strong&gt;进行充满信心的猜测&lt;/strong&gt;，但&lt;strong&gt;永远无法&lt;/strong&gt;通过统计&lt;strong&gt;得出确定无疑的结果&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;第一部分：搜集统计数据&quot;&gt;&lt;a href=&quot;#第一部分：搜集统计数据&quot; class=&quot;headerlink&quot; title=&quot;第一部分：搜集统计数据&quot;&gt;&lt;/a&gt;第一部分：搜集统计数据&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;数字&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;统计并不只关系到数字，统计关系到我们的&lt;strong&gt;信心&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;在处理任何数字时，都带着&lt;strong&gt;适当的怀疑&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;随机原始数据&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;用样本描述总体可谓&lt;strong&gt;聪明之举&lt;/strong&gt;，不过在&lt;strong&gt;尝试迈出实质性的一步&lt;/strong&gt;之前，我们要记住几个事实。首先，我们&lt;strong&gt;不可能&lt;/strong&gt;通过样本&lt;strong&gt;完全确定&lt;/strong&gt;一个总体。统计指的是做出&lt;strong&gt;最佳猜测&lt;/strong&gt;，而&lt;strong&gt;绝非确凿无疑&lt;/strong&gt;的判断。其次，我们在采集样本时犯下的&lt;strong&gt;任何错误&lt;/strong&gt;，都能&lt;strong&gt;彻底歪曲&lt;/strong&gt;我们对较大总体的结论。&lt;/li&gt;
&lt;li&gt;采集样本&lt;strong&gt;最大的挑战&lt;/strong&gt;可能在于，准确指出要在样本中&lt;strong&gt;包含哪些内容&lt;/strong&gt;。目标是避免&lt;strong&gt;样本中出现偏倚&lt;/strong&gt;，偏倚可能会导致我们&lt;strong&gt;曲解总体&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;理想情况是，我们采集的样本能&lt;strong&gt;正确反映总体&lt;/strong&gt;，为了避免偏倚，我们&lt;strong&gt;总是随机&lt;/strong&gt;采集样本。&lt;/li&gt;
&lt;li&gt;随机样本效果显著的原因是，它表明我们抽取任何一个样本的&lt;strong&gt;可能性&lt;/strong&gt;都和抽取其他样本的&lt;strong&gt;可能性是一样大&lt;/strong&gt;；如果这些样本有差别，这&lt;strong&gt;纯属偶然&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;因为&lt;strong&gt;随机抽样&lt;/strong&gt;是一切统计调查的&lt;strong&gt;关键&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;排序&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;两种数据:类别型数据和&lt;strong&gt;数值型&lt;/strong&gt;数据，我们不能对类别数据进行&lt;strong&gt;数学运算&lt;/strong&gt;，但可以对数值型数据进行数学运算。&lt;/li&gt;
&lt;li&gt;箱线图，把位于样本&lt;strong&gt;中间的50%数&lt;/strong&gt;值挤在&lt;strong&gt;一个大箱子&lt;/strong&gt;里，然后分别指出最小值、中间值和最大值。&lt;/li&gt;
&lt;li&gt;通常，当我们希望对整个数据集进行&lt;strong&gt;整体描述&lt;/strong&gt;时，我们画&lt;strong&gt;直方图&lt;/strong&gt;，包含&lt;strong&gt;精确的细节&lt;/strong&gt;；另一方面，如果我们希望概要了解数据的&lt;strong&gt;情况&lt;/strong&gt;或者希望&lt;strong&gt;对不同样本或群组进行&lt;/strong&gt;比较，这时&lt;strong&gt;箱线图&lt;/strong&gt;特别管用。&lt;/li&gt;
&lt;li&gt;箱线图仿佛&lt;strong&gt;缩小版&lt;/strong&gt;的直方图，可以让我们迅速了解&lt;strong&gt;数据的汇聚情况&lt;/strong&gt;，以及数据是否朝着一端或另一端&lt;strong&gt;延展&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;尽管我们会受到&lt;strong&gt;更具奥妙的数学工具诱惑&lt;/strong&gt;，但简单的图形却能让我们集中关注数据&lt;strong&gt;表达的确切意义&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;侦探工作&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;当我们&lt;strong&gt;动手调查&lt;/strong&gt;任何一批数据时，总是观察&lt;strong&gt;四个主要特性&lt;/strong&gt;：样本大小，形状，位置，分散性。&lt;/li&gt;
&lt;li&gt;一般来说，样本&lt;strong&gt;越大&lt;/strong&gt;，结果&lt;strong&gt;越好&lt;/strong&gt;！样本大小会&lt;strong&gt;直接关系&lt;/strong&gt;到我们对一个总体可以具有的&lt;strong&gt;置信水平&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;无论数据汇聚成什么形状（平的，正态的，偏斜的），总是&lt;strong&gt;有其原因的&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;位置是对一批数据在&lt;strong&gt;一个数轴上的落点&lt;/strong&gt;的度量，我们常用一个数字&lt;strong&gt;平均数&lt;/strong&gt;来描述位置，但如果数据是偏斜的，那么度量位置时&lt;strong&gt;中位数&lt;/strong&gt;更能体现数据情况。&lt;/li&gt;
&lt;li&gt;分散性是对&lt;strong&gt;一批数据的宽度&lt;/strong&gt;的度量，同时也是对&lt;strong&gt;变异性&lt;/strong&gt;的度量。&lt;/li&gt;
&lt;li&gt;测量分散性的一个简单明了的办法是&lt;strong&gt;算出全距&lt;/strong&gt;，即最大值与最小值的差值，然后&lt;strong&gt;分成四块&lt;/strong&gt;，中间的两块被称为&lt;strong&gt;四分位距（IQR）&lt;/strong&gt; （箱线图）。&lt;/li&gt;
&lt;li&gt;分散性的最常见度量方法是&lt;strong&gt;标准差（SD）&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;怪异的错误&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;有两个峰的直方图形状称为双峰。&lt;/li&gt;
&lt;li&gt;潜在变量会给&lt;strong&gt;各种统计分析带来麻烦&lt;/strong&gt;，统计师的部分工作就是&lt;strong&gt;发现潜在变量&lt;/strong&gt;。（只吃卷心菜会让人长寿，只吃卷心菜的人往往锻炼得更加勤快）&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;从样本到总体&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;在通用数学术语中，“分布”一词描述了一个随机变量得所有可能数值的位置情况。&lt;/li&gt;
&lt;li&gt;统计的目的是利用样本对总体进行猜测，一向如此。我们为二者使用不同的术语和技术符号。&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:center&quot;&gt;我们将统计特性称为“统计值”&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;我们将总体特性称为“参数”&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;书写公式时，“x拔”特指&lt;strong&gt;样本平均数&lt;/strong&gt; $\mathbf{\overline{x}}$&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;小写希腊字母“缪”特指&lt;strong&gt;总体平均数&lt;/strong&gt;：$\mathbf{\mu}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;s特指&lt;strong&gt;样本标准差&lt;/strong&gt;：$\mathbf{s}$&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;小写希腊字母“西格玛”特指&lt;strong&gt;总体标准差&lt;/strong&gt;：$\sigma$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;&lt;strong&gt;统计值&lt;/strong&gt;是我们实际测量的数值，因此是确凿无疑的数值。&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;&lt;strong&gt;参数&lt;/strong&gt;是我们想知道的数值，但只能通过猜测获得。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;换句话说，我们&lt;strong&gt;采集统计值&lt;/strong&gt;的唯一原因，是我们&lt;strong&gt;对参数有兴趣&lt;/strong&gt;。尽管我们&lt;strong&gt;永远&lt;/strong&gt;做不到&lt;strong&gt;直接观察参数&lt;/strong&gt;，却可以&lt;strong&gt;利用统计值&lt;/strong&gt;去&lt;strong&gt;发掘参数&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;我们即将学习利用从&lt;strong&gt;随机样本&lt;/strong&gt;算出的统计值，挖掘作为&lt;strong&gt;样本来源总体的平均数&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;第二部分：探寻参数&quot;&gt;&lt;a href=&quot;#第二部分：探寻参数&quot; class=&quot;headerlink&quot; title=&quot;第二部分：探寻参数&quot;&gt;&lt;/a&gt;第二部分：探寻参数&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;中心极限定理&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;让我们设想：我们希望知道&lt;strong&gt;某个总体的平均值&lt;/strong&gt;，然后外出并&lt;strong&gt;从该总体中采集了一连串各自独立的随机样本&lt;/strong&gt;。结果证明，如果我们&lt;strong&gt;算出每个随机样本的平均数&lt;/strong&gt;，然后把这些平均数&lt;strong&gt;按顺序堆起来&lt;/strong&gt;，随着堆放的样本平均数&lt;strong&gt;越来越多&lt;/strong&gt;，堆成的外形&lt;strong&gt;越趋向于正态&lt;/strong&gt;。从技术上看，像这样一个巨大的堆积形状是一类&lt;strong&gt;抽样分布&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;这个发现对于出自&lt;strong&gt;任何总体&lt;/strong&gt;的随机样本平均数都成立。至于&lt;strong&gt;总体本身&lt;/strong&gt;是什么形状，倒&lt;strong&gt;并不要紧&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;中心极限定理（CLT）是许多统计推论的依据，它指出：当样本大小 $n$ 很大时，$\overline{x}$ 的抽样分布接近&lt;strong&gt;正态&lt;/strong&gt;。更具体地说，对于从单一总体（平均数为 $\mu$，标准差为 $\sigma$ ）中抽取的大小为 $n$ 的多个大型&lt;strong&gt;随机样本&lt;/strong&gt;，$\overline{x}$ 的分布近似于平均数为 $\mu$，标准差为 $\sigma / \sqrt{n}$ 的正态分布。&lt;/li&gt;
&lt;li&gt;中心极限定理&lt;strong&gt;只有&lt;/strong&gt;在每个样本均为&lt;strong&gt;随机&lt;/strong&gt;抽取&lt;strong&gt;且&lt;/strong&gt;每个样本都&lt;strong&gt;足够大&lt;/strong&gt;（达到30以上被认为是大样本，通常就够了）时才成立。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;概率&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;抽样分布的两个奥妙：它能告诉我们&lt;strong&gt;总体均值&lt;/strong&gt;，我们可以利用它&lt;strong&gt;算出总体的概率&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;正态分布&lt;br&gt;位于距离中心值1个标准差的范围内，68%&lt;br&gt;位于距离中心值2个标准差的范围内，95%&lt;br&gt;位于距离中心值3个标准差的范围内，99.7%&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;推断&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;可惜，在现实生活中，我们&lt;strong&gt;绝不会弄一个实际的抽样分布进行观察&lt;/strong&gt;，我们所拥有的不过是&lt;strong&gt;一罐&lt;/strong&gt;而已。&lt;/li&gt;
&lt;li&gt;由于&lt;strong&gt;样本平均数&lt;/strong&gt;倾向于&lt;strong&gt;簇拥在总体平均数周围&lt;/strong&gt;，我们可以画一堆数，用来&lt;strong&gt;猜测总体平均数的位置&lt;/strong&gt;，统计学家们把这样的过程称为&lt;strong&gt;推断&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;估计抽样分布：在实践中，当使用中心极限定理时，我们无法获知参数 $\mu$ 和 $\sigma$ 的真实值，因此我们用统计值 $\overline{x}$ 和 $s$ 来近似 $\mu$ 和 $\sigma$。这种&lt;strong&gt;近似&lt;/strong&gt;之所以可行，是因为我们的统计值是随机的。结果，我们期望 $\overline{x}$ 与 $\mu$ 有差别，$s$ 和 $\sigma$ 有差别，但这&lt;strong&gt;只是因为偶然变异&lt;/strong&gt;。代入近似值后，我们将结果称为&lt;strong&gt;估计抽样分布&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;信心&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;如果你取得一个大小合适的&lt;strong&gt;随机样本&lt;/strong&gt;，并用这个样本&lt;strong&gt;构建一个估计抽样分布&lt;/strong&gt;，然后&lt;strong&gt;从距离中心2个标准差的地方减去尾部&lt;/strong&gt;，那么&lt;strong&gt;95%的情况下&lt;/strong&gt;你都会得到一个包含&lt;strong&gt;真正的总体平均数的范围&lt;/strong&gt;！换句话说，我们有95%的信心总体平均数在这个范围内的某个地方。&lt;/li&gt;
&lt;li&gt;置信区间：从技术上说&lt;strong&gt;置信区间&lt;/strong&gt;是与特定&lt;strong&gt;置信水平&lt;/strong&gt;有关的一类区间估计。我们可以计算任何参数的置信区间，但特定技术细节有所差别。&lt;/li&gt;
&lt;li&gt;理想情况下，我们希望任何置信水平的相应置信区间都&lt;strong&gt;尽量最窄&lt;/strong&gt;，因为置信区间越窄越精确。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;恨之深&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;增大 $n$ （通过采集更多观测值）是获得更窄区间的可靠办法，这正是样本越大越好的原因！&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;假设检验&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;首先，我们用&lt;strong&gt;一个样本构建一个估计抽样分布&lt;/strong&gt;，然后把这个分布&lt;strong&gt;推到另一个我们有兴趣研究的位置&lt;/strong&gt;。接下来，我们&lt;strong&gt;回顾&lt;/strong&gt;通过我们的唯一样本得知的平均数，并提问：如果总体平均数&lt;strong&gt;确实在这儿&lt;/strong&gt;，那么我们随机抽取一个和我们手头的样本一样的样本的&lt;strong&gt;可能性有多大&lt;/strong&gt;？&lt;/li&gt;
&lt;li&gt;P值：在原假设为真的前提下，我们将会观测到的数据的极值不超过我们实际观测到的数据的极值（双尾检验，单尾检验）的概率。&lt;/li&gt;
&lt;li&gt;假设检验总是以算出P值并据此做出&lt;strong&gt;正式结论&lt;/strong&gt;为终点。这个&lt;strong&gt;正式结论&lt;/strong&gt;指出：我们是否认为我们的统计值与&lt;strong&gt;原假设&lt;/strong&gt;所预测饿的参数具有足够大的偏差，从而可以有理有据地&lt;strong&gt;拒绝&lt;/strong&gt;原假设，而选择另一观点。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;破立之争&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;在实践中，我们将一种假设（样本）与&lt;strong&gt;原假设&lt;/strong&gt;进行比较。不过有一些&lt;strong&gt;偏心的规矩&lt;/strong&gt;，原假设总想赢，除非有足够强的证据证明原假设不对！&lt;/li&gt;
&lt;li&gt;假设检验的要点是，断不可&lt;strong&gt;妄下结论&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;走向高级：飞猪、外星口水虫和焰火&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;差值推断&lt;/li&gt;
&lt;li&gt;小样本推断：T分布&lt;/li&gt;
&lt;li&gt;标准差推断&lt;/li&gt;
&lt;li&gt;本质上，一切统计问题都相似。这些问题看起来就像这样：如果我们只能得到&lt;strong&gt;样本&lt;/strong&gt;，如何对&lt;strong&gt;总体&lt;/strong&gt;做出判断？我们解决的办法则像这样：我们利用手头的数据估计出某种&lt;strong&gt;抽样分布&lt;/strong&gt;，然后&lt;strong&gt;截取它的概率&lt;/strong&gt;，但有时候先把这个分布&lt;strong&gt;推移到一个新位置&lt;/strong&gt;更有用。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;结语：像统计师一样思考&quot;&gt;&lt;a href=&quot;#结语：像统计师一样思考&quot; class=&quot;headerlink&quot; title=&quot;结语：像统计师一样思考&quot;&gt;&lt;/a&gt;结语：像统计师一样思考&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;在&lt;strong&gt;第一部分&lt;/strong&gt;，我们介绍了样本数据的&lt;strong&gt;堆积方式&lt;/strong&gt;（形状，位置和分布），并进行了研究。接着，在&lt;strong&gt;第二部分&lt;/strong&gt;，我们学习了&lt;strong&gt;统计&lt;/strong&gt;推断，也就是如何&lt;strong&gt;利用样本找出整个总体的特性&lt;/strong&gt;。我们特别学习了如何构建&lt;strong&gt;估计抽样分布&lt;/strong&gt;，如何&lt;strong&gt;剪裁&lt;/strong&gt;，以便计算&lt;strong&gt;置信区间&lt;/strong&gt;，或推移到附近，以便进行&lt;strong&gt;假设检验&lt;/strong&gt;。最后，我们学习了如何在问题变得&lt;strong&gt;更为复杂&lt;/strong&gt;时，修正这些基本步骤。&lt;/li&gt;
&lt;li&gt;我们通过这本书学习了&lt;strong&gt;统计学家的思维方式&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;寒假看了一本《深入浅出统计学》，觉得讲得很好，整理出阅读笔记。&lt;br&gt;
    
    </summary>
    
      <category term="Math" scheme="http://songbinbin.me/categories/Math/"/>
    
    
      <category term="Reading Notes" scheme="http://songbinbin.me/tags/Reading-Notes/"/>
    
      <category term="Statistics" scheme="http://songbinbin.me/tags/Statistics/"/>
    
  </entry>
  
  <entry>
    <title>东詹与西詹</title>
    <link href="http://songbinbin.me/2017/01/13/east-and-west-james/"/>
    <id>http://songbinbin.me/2017/01/13/east-and-west-james/</id>
    <published>2017-01-13T03:12:41.000Z</published>
    <updated>2017-01-13T03:39:26.259Z</updated>
    
    <content type="html">&lt;p&gt;今年开始黑转路转粉的两位球员，勒布朗詹姆斯与詹姆斯哈登。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;之前一直不太喜欢詹姆斯，或许是他总被用来和蜗壳做比较的缘故吧。觉得他太功利啦，打球不具有美感啦之类云云。但自从他去年夺冠以后，逐渐开始喜欢这位，也许可以竞争NBA史上最伟大球员的总统山人物。总冠军固然有加成，但我真正看中的，是他的领袖气质，尤其是带动队友不断变好的能力。这点是很多成功人物所缺乏的，在保持自己绝对实力的同时，也能让周围人也变得更出色。&lt;/p&gt;
&lt;p&gt;另一位，之前总喜欢逛夜店、场上眼神防守的登哥。去年休赛期，在签下一份顶薪合同后，一点负面新闻没有，全是刻苦训练的报导。当然另外也有签下顶薪之后更加努力的球员，与新的战术体系也有关系，但哈登这个赛季的转变，场上的表现、赛后的话语，真是让我对他刮目相看。到底是什么让一个人产生如此蜕变？不得而知。我觉得还是他内在的原动力，源于他从小到大所经历的一切，终于在这个时间点，让他厚积薄发。&lt;/p&gt;
&lt;p&gt;联盟即世界，赛场即人生，两位詹姆斯，继续加油。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;今年开始黑转路转粉的两位球员，勒布朗詹姆斯与詹姆斯哈登。&lt;br&gt;
    
    </summary>
    
      <category term="NBA" scheme="http://songbinbin.me/categories/NBA/"/>
    
    
      <category term="diary" scheme="http://songbinbin.me/tags/diary/"/>
    
      <category term="harden" scheme="http://songbinbin.me/tags/harden/"/>
    
      <category term="james" scheme="http://songbinbin.me/tags/james/"/>
    
  </entry>
  
  <entry>
    <title>随记（一）</title>
    <link href="http://songbinbin.me/2017/01/12/diary-preparing-for-work/"/>
    <id>http://songbinbin.me/2017/01/12/diary-preparing-for-work/</id>
    <published>2017-01-12T13:21:32.000Z</published>
    <updated>2017-01-13T03:13:17.943Z</updated>
    
    <content type="html">&lt;p&gt;2017年1月12日，研二上学期即将结束，实习一步步准备，最后一次组会，论文小修回复。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;最近都在准备找实习，算是过得挺充实吧。每天就是看算法，看机器学习，刷LeetCode。游泳卡用了47次，挽救了两个月，结果还行。&lt;/p&gt;
&lt;p&gt;科研生涯就这么结束了，当初选了深度学习这个大坑，到头来也就趟了一趟浑水。主要是深度学习这几年往前走得太快了，真正的沉淀还不够，作为课题确实不是一个好选择；如果单纯是拿来应用，那确实是一项利器，只是只用来应用的人太多了吧。&lt;/p&gt;
&lt;p&gt;不过，跟过这波热潮后，也算得到点启发。若是真想学好某一方面知识，希望以后能在这一领域有所作为，草草地过一遍绝对是不行的。现在的生活节奏太快，也会让我们变得浮躁。在准备找工作的这段时间是个好机会，希望自己能静下心来，好好补一下基础知识，不能光顾着眼前利益。&lt;/p&gt;
&lt;p&gt;这段时间和QQ交流过几次，最近的生活虽然累点，但也自由，算是“无欲无求”了。希望通过这段时间能真正找到自己所喜欢做的事情，让以后自己的精神做到自由。若是将来后的某一天被问起，“最近想要些什么，要不要换换环境放松一下”，自己还能从心底说出，“不了，现在的生活就是我梦寐以求的”。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;2017年1月12日，研二上学期即将结束，实习一步步准备，最后一次组会，论文小修回复。&lt;br&gt;
    
    </summary>
    
      <category term="Life" scheme="http://songbinbin.me/categories/Life/"/>
    
    
      <category term="diary" scheme="http://songbinbin.me/tags/diary/"/>
    
  </entry>
  
  <entry>
    <title>我家QQ</title>
    <link href="http://songbinbin.me/2016/11/24/myQQ/"/>
    <id>http://songbinbin.me/2016/11/24/myQQ/</id>
    <published>2016-11-24T14:03:10.000Z</published>
    <updated>2016-11-24T14:10:16.862Z</updated>
    
    <content type="html">&lt;p&gt;日常等QQ，不小心拍到我Q正好换了一身新衣裳，美美哒～&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/EgGWJhkzZoI&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;</content>
    
    <summary type="html">
    
      &lt;p&gt;日常等QQ，不小心拍到我Q正好换了一身新衣裳，美美哒～&lt;br&gt;
    
    </summary>
    
      <category term="Life" scheme="http://songbinbin.me/categories/Life/"/>
    
    
      <category term="QQ" scheme="http://songbinbin.me/tags/QQ/"/>
    
      <category term="video" scheme="http://songbinbin.me/tags/video/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop实验--带词频的文档倒排索引</title>
    <link href="http://songbinbin.me/2016/11/08/Hadoop-InvertedIndex/"/>
    <id>http://songbinbin.me/2016/11/08/Hadoop-InvertedIndex/</id>
    <published>2016-11-08T01:37:27.000Z</published>
    <updated>2016-11-08T02:52:18.951Z</updated>
    
    <content type="html">&lt;p&gt;Inverted Index（倒排索引）是目前几乎所有支持全文检索的搜索引擎都要依赖的一个数据结构。基于索引结构，给出一个词(term)，能取得含有这个term的文档列表(the list of documents)。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;1-简单的文档倒排索引&quot;&gt;&lt;a href=&quot;#1-简单的文档倒排索引&quot; class=&quot;headerlink&quot; title=&quot;1. 简单的文档倒排索引&quot;&gt;&lt;/a&gt;1. 简单的文档倒排索引&lt;/h2&gt;&lt;p&gt;倒排索引最简单的形式，只统计词(term)所出现的文档。&lt;br&gt;Map和Reduce实现伪代码：&lt;/p&gt;
&lt;p&gt;$ 1: class\ Mapper $&lt;br&gt;$ 2:&amp;emsp; procedure\ Map(docid\ dn,\ doc\ d) $&lt;br&gt;$ 3:&amp;emsp;&amp;emsp; for\ all\ term\ t \in doc\ d\ do $&lt;br&gt;$ 4:&amp;emsp;&amp;emsp;&amp;emsp; Emit(term\ t,\ doc\ d) $&lt;/p&gt;
&lt;p&gt;$ 1: class \  Reducer $&lt;br&gt;$ 2:&amp;emsp; procedure\ Reduce(term\ t,\ Iter_d \langle d_1,d_2,…,d_n \rangle ) $&lt;br&gt;$ 3:&amp;emsp;&amp;emsp; H\leftarrow new\ AssociativeArray $&lt;br&gt;$ 4:&amp;emsp;&amp;emsp; P\leftarrow new\ List $&lt;br&gt;$ 5:&amp;emsp;&amp;emsp; for\ all\ d \in Iter_d\ do $&lt;br&gt;$ 6:&amp;emsp;&amp;emsp;&amp;emsp; if\ d\ not\ in\ H $&lt;br&gt;$ 7:&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp; H\{d\}\leftarrow 1 $&lt;br&gt;$ 8:&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp; P.append(d) $&lt;br&gt;$ 9:&amp;emsp;&amp;emsp; Emit(term \ t, \ List\ P) $&lt;/p&gt;
&lt;h2 id=&quot;2-带词频的文档倒排索引&quot;&gt;&lt;a href=&quot;#2-带词频的文档倒排索引&quot; class=&quot;headerlink&quot; title=&quot;2. 带词频的文档倒排索引&quot;&gt;&lt;/a&gt;2. 带词频的文档倒排索引&lt;/h2&gt;&lt;p&gt;除了统计词(term)所出现的文档外，我们还想统计其在每个文档中出现的次数。我们把文档以及次数称作一个posting，这样每个词(term)的倒排索引即由很多个posting即一个posting list组成。&lt;br&gt;伪代码如下：&lt;/p&gt;
&lt;p&gt;$ 1: class\ Mapper $&lt;br&gt;$ 2:&amp;emsp; procedure\ Map(docid\ dn,\ doc\ d) $&lt;br&gt;$ 3:&amp;emsp;&amp;emsp; F\leftarrow new\ AssociativeArray $&lt;br&gt;$ 4:&amp;emsp;&amp;emsp; for\ all\ term\ t\in doc\ d\ do $&lt;br&gt;$ 5:&amp;emsp;&amp;emsp;&amp;emsp; F\{t\}\leftarrow F\{t\}\ +\ 1 $&lt;br&gt;$ 6:&amp;emsp;&amp;emsp; for\ all\ term\ t\in F\ do $&lt;br&gt;$ 7:&amp;emsp;&amp;emsp;&amp;emsp; Emit(term\ t,\ posting\langle doc\ d,\ F\{t\}\rangle ) $&lt;/p&gt;
&lt;p&gt;$ 1: class \  Reducer $&lt;br&gt;$ 2:&amp;emsp; procedure\ Reduce(term\ t,\ postings[\langle d_1,f_1\rangle ,\langle d_2,f_2\rangle ,…])\ do  $&lt;br&gt;$ 3:&amp;emsp;&amp;emsp; H\ \leftarrow\ new\ AssociativeArray $&lt;br&gt;$ 4:&amp;emsp;&amp;emsp; P\ \leftarrow\ new\ List $&lt;br&gt;$ 5:&amp;emsp;&amp;emsp; for\ all\ posting \in postings[\langle d_1,f_1\rangle ,\langle d_2,f_2\rangle ,…]\ do $&lt;br&gt;$ 6:&amp;emsp;&amp;emsp;&amp;emsp; H\{d_n\} \leftarrow H\{d_n\}\ +\ f_n $&lt;br&gt;$ 7:&amp;emsp;&amp;emsp; for\ all\ d\in H\ do $&lt;br&gt;$ 8:&amp;emsp;&amp;emsp;&amp;emsp; P.append(\langle d,\ H\{d\} \rangle ) $&lt;br&gt;$ 9:&amp;emsp;&amp;emsp; Emit(term \ t, \ List\ P) $&lt;/p&gt;
&lt;h2 id=&quot;3-改进的带词频的文档倒排索引&quot;&gt;&lt;a href=&quot;#3-改进的带词频的文档倒排索引&quot; class=&quot;headerlink&quot; title=&quot;3. 改进的带词频的文档倒排索引&quot;&gt;&lt;/a&gt;3. 改进的带词频的文档倒排索引&lt;/h2&gt;&lt;p&gt;上面的倒排索引算法存在一定的缺陷:Mapper的输出即Reducer的输入的key值是某一个词(term),某些词在所有文件中可能会出现很多次，这样可能会导致reduce节点的内存溢出。&lt;br&gt;一个解决办法是将Mapper输出的key值修改为词加文件名(term,doc)，value值为出现次数。这样带来的新的问题是同样的词可能会被Partitioner分配到不同的reduce节点，对此，我们可以定制自己的Partitioner来解决，让其按照term进行分区。&lt;br&gt;伪代码如下：&lt;/p&gt;
&lt;p&gt;$ 1: class\ Mapper $&lt;br&gt;$ 2:&amp;emsp; procedure\ Map(docid\ dn,\ doc\ d) $&lt;br&gt;$ 3:&amp;emsp;&amp;emsp; for\ all\ term\ t \in doc\ d\ do $&lt;br&gt;$ 4:&amp;emsp;&amp;emsp;&amp;emsp; Emit(\langle t,doc \rangle ,\ 1) $&lt;/p&gt;
&lt;p&gt;$ 1: class \  Combiner $&lt;br&gt;$ 2:&amp;emsp; procedure\ Combine(pair(t,d),\ counts[f_1,f_2,…])\ do  $&lt;br&gt;$ 3:&amp;emsp;&amp;emsp; s\ \leftarrow\ 0 $&lt;br&gt;$ 4:&amp;emsp;&amp;emsp; for\ all\ count\ f\ in\ counts\ [f_1,f_2,…]\ do $&lt;br&gt;$ 5:&amp;emsp;&amp;emsp;&amp;emsp; s\ \leftarrow\ s\ +\ f $&lt;br&gt;$ 6:&amp;emsp;&amp;emsp; Emit(pair(t,d), s) $&lt;/p&gt;
&lt;p&gt;$ 1: class\ Partitioner $&lt;br&gt;$ 2: &amp;emsp; procedure\ Partition(key\ \langle t,doc \rangle ,\ value\ f) $&lt;br&gt;$ 3: &amp;emsp;&amp;emsp; getPartition(key\ t,\ value) $&lt;/p&gt;
&lt;p&gt;$ 1: class \  Reducer $&lt;br&gt;$ 2:&amp;emsp; procedure\ Reduce(pairs[\langle t,d_1\rangle ,\langle t,d_2\rangle ,…  ],\ counts[f_1,f_2,…])\ do  $&lt;br&gt;$ 3:&amp;emsp;&amp;emsp; P\ \leftarrow\ new\ List $&lt;br&gt;$ 4:&amp;emsp;&amp;emsp; for\ all\ doc\ d\ in\ pairs[\langle t,d_1\rangle ,\langle t,d_2\rangle ,…  ]\ do $&lt;br&gt;$ 5:&amp;emsp;&amp;emsp;&amp;emsp; s\ \leftarrow\ 0 $&lt;br&gt;$ 6:&amp;emsp;&amp;emsp;&amp;emsp; for\ all\ count\ f\ in\ counts\ [f_1,f_2,…]\ do $&lt;br&gt;$ 7:&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp; s\ \leftarrow\ s\ +\ f $&lt;br&gt;$ 8:&amp;emsp;&amp;emsp;&amp;emsp; P.append(\langle d,s \rangle ) $&lt;br&gt;$ 9:&amp;emsp;&amp;emsp; P.insert(0,\ mean(P.s)) $&lt;br&gt;$ 10:&amp;ensp;&amp;emsp; Emit(term \ t, \ List\ P) $&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;Inverted Index（倒排索引）是目前几乎所有支持全文检索的搜索引擎都要依赖的一个数据结构。基于索引结构，给出一个词(term)，能取得含有这个term的文档列表(the list of documents)。&lt;br&gt;
    
    </summary>
    
      <category term="Software" scheme="http://songbinbin.me/categories/Software/"/>
    
    
      <category term="Hadoop" scheme="http://songbinbin.me/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>wuliQQ</title>
    <link href="http://songbinbin.me/2016/11/02/wuliQQ/"/>
    <id>http://songbinbin.me/2016/11/02/wuliQQ/</id>
    <published>2016-11-02T01:34:05.000Z</published>
    <updated>2016-11-07T09:43:53.045Z</updated>
    
    <content type="html">&lt;p&gt;想做这个视频很久了，wuliQQ，不愿天长地久，但愿曾经拥有。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/hLIGaPoyIyk&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;想做这个视频很久了，wuliQQ，不愿天长地久，但愿曾经拥有。&lt;br&gt;
    
    </summary>
    
      <category term="Life" scheme="http://songbinbin.me/categories/Life/"/>
    
    
      <category term="QQ" scheme="http://songbinbin.me/tags/QQ/"/>
    
      <category term="video" scheme="http://songbinbin.me/tags/video/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop实验--WordCount</title>
    <link href="http://songbinbin.me/2016/10/24/Hadoop-WordCount/"/>
    <id>http://songbinbin.me/2016/10/24/Hadoop-WordCount/</id>
    <published>2016-10-24T13:59:37.000Z</published>
    <updated>2016-11-08T01:41:01.999Z</updated>
    
    <content type="html">&lt;p&gt;今天给大家带来Hadoop下的第一个Hello World程序。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;1-选用文本&quot;&gt;&lt;a href=&quot;#1-选用文本&quot; class=&quot;headerlink&quot; title=&quot;1. 选用文本&quot;&gt;&lt;/a&gt;1. 选用文本&lt;/h1&gt;&lt;p&gt;实验所选用的数据选用的是《冰与火之歌》(A Song of Ice and Fire)的英文txt文件，从下面这个网站中下载：&lt;a href=&quot;http://persischempaka.blogspot.com/2012/04/game-of-thronestxt.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://persischempaka.blogspot.com/2012/04/game-of-thronestxt.html&lt;/a&gt;&lt;br&gt;一共有五个txt文件，分别是:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;001ssb.txt (A Game of Thrones，权力的游戏，1.6M)&lt;/li&gt;
&lt;li&gt;002ssb.txt (A Clash of Kings，列王的纷争，1.8M)&lt;/li&gt;
&lt;li&gt;003ssb.txt (A Storm of Swords，冰与的风暴，2.4M)&lt;/li&gt;
&lt;li&gt;004ssb.txt (A Feast for Crows，群鸦的盛宴，1.7M)&lt;/li&gt;
&lt;li&gt;005ssb.txt (A Dance with Dragons，魔龙的狂舞，2.4M)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;2-创建HDFS文件目录&quot;&gt;&lt;a href=&quot;#2-创建HDFS文件目录&quot; class=&quot;headerlink&quot; title=&quot;2. 创建HDFS文件目录&quot;&gt;&lt;/a&gt;2. 创建HDFS文件目录&lt;/h1&gt;&lt;p&gt;格式化文件系统并启动，因为伪分布式读取的是HDFS上的数据，要执行MapReduce任务，我们需要创建HDFS文件系统。使用HDFS文件操作命令：&lt;code&gt;hdfs dfs -mkdir －p /user/tyrion&lt;/code&gt;，其中，&lt;code&gt;tyrion&lt;/code&gt;是你当前Ubuntu的用户名。该命令的行为与UNIX下&lt;code&gt;mkdir -p&lt;/code&gt;相似，这一路径上的父目录如果不存在，则创建该父目录。如果按如上命令，那么现在就默认在HDFS文件系统中的&lt;code&gt;/user/tyrion&lt;/code&gt;目录下。接下来创建&lt;code&gt;input&lt;/code&gt;文件夹用于存放输入数据，使用put命令将之前的txt文件拷入input文件夹中。类似，ls命令可以查看文件列表。&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;hdfs dfs -mkdir input&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;hdfs dfs -put /path_to_txt_file/*.txt input&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;hdfs dfs -ls input&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;在浏览器中输入&lt;code&gt;localhost:50070&lt;/code&gt;可以查看集群基本信息，包括文件信息。&lt;br&gt;&lt;img src=&quot;/img/hadoop/file_list.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;3-使用IntelliJ编写Hadoop程序&quot;&gt;&lt;a href=&quot;#3-使用IntelliJ编写Hadoop程序&quot; class=&quot;headerlink&quot; title=&quot;3. 使用IntelliJ编写Hadoop程序&quot;&gt;&lt;/a&gt;3. 使用IntelliJ编写Hadoop程序&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;新建一个Project，一路默认。&lt;/li&gt;
&lt;li&gt;添加依赖，依次打开&lt;code&gt;File - Project Structure - Modules - (右侧)+ - Library Type - Java&lt;/code&gt;。选择hadoop目录下&lt;code&gt;share/hadoop&lt;/code&gt;中的文件夹，本次实验WordCount只用到common和mapreduce文件夹，所以只添加这两个就行了。&lt;/li&gt;
&lt;li&gt;在src下新建一个WordCount类，代码可参考官网Documentation中的Tutorial，在此不做赘述。&lt;/li&gt;
&lt;li&gt;导出jar包。依次选择&lt;code&gt;File - Project Structure - Artifacts - + - JAR - From modules with dependencies...&lt;/code&gt;，选择要Main Class，确定。接下来选择菜单&lt;code&gt;Build - Build Artifacts - Build&lt;/code&gt;即可，生成的jar文件位于工程项目目录的&lt;code&gt;out/artifacts&lt;/code&gt;下。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&quot;4-运行Hadoop程序&quot;&gt;&lt;a href=&quot;#4-运行Hadoop程序&quot; class=&quot;headerlink&quot; title=&quot;4. 运行Hadoop程序&quot;&gt;&lt;/a&gt;4. 运行Hadoop程序&lt;/h1&gt;&lt;p&gt;可以在hadoop下创建一个exercise文件夹，将刚才生成的jar包拷到其中，按如下命令运行:&lt;br&gt;&lt;code&gt;hadoop jar ./exercise/wc.jar input output&lt;/code&gt;&lt;br&gt;使用命令&lt;code&gt;hdfs dfs -cat output/*&lt;/code&gt;可以查看运行结果。&lt;br&gt;通过命令&lt;code&gt;hdfs dfs -get output ./test_out&lt;/code&gt;可以将结果取回本地。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;运行Hadoop程序时，为了防止覆盖结果，程序指定的输出目录（如output）不能存在，否则会提示错误。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;也可以使用Intellij结合Maven本地运行和调试MapReduce程序，该方法最大的特点是不需要安装任何模式的Hadoop，只要在Maven配置文件中指定Hadoop依赖包名字和版本号，Maven就能自动搞定这些依赖。不过刚开始我还是偏向用IntelliJ以及命令行来，以后再来详细研究。&lt;/p&gt;
&lt;h1 id=&quot;5-在Web界面查看作业运行状态&quot;&gt;&lt;a href=&quot;#5-在Web界面查看作业运行状态&quot; class=&quot;headerlink&quot; title=&quot;5. 在Web界面查看作业运行状态&quot;&gt;&lt;/a&gt;5. 在Web界面查看作业运行状态&lt;/h1&gt;&lt;p&gt;要在Web端查看作业的执行情况，需要启动YARN，让YARN来负责资源管理与任务调度。新版Hadoop使用了新的MapReduce框架(YARN，Yet Another Resource Negotiator)。YARN是从MapReduce中分离出来，负责资源管理与任务调度。YARN运行于MapReduce之上，提供了高可用性、高扩展性。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先修改配置文件mapred-site.xml，原始只有.template文件，这边需要先进行重命名：&lt;br&gt;&lt;code&gt;mv ./etc/hadoop/mapred-site.xml.template ./etc/hadoop/mapred-site.xml&lt;/code&gt;&lt;blockquote&gt;
&lt;p&gt;不启动YARN时需改回.template后缀，否则在该配置文件存在，而未开启YARN的情况下会一直尝试连接。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;修改mapred-site.xml以及yarn-site.xm文件。&lt;/p&gt;
&lt;figure class=&quot;highlight xml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;configuration&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;mapreduce.framework.name&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;yarn&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;configuration&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
 &lt;figure class=&quot;highlight xml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;configuration&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;yarn.nodemanager.aux-services&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;mapreduce_shuffle&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;configuration&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;启动Hadoop &lt;code&gt;./sbin/start-dfs.sh&lt;/code&gt;&lt;br&gt;启动YARN &lt;code&gt;./sbin/start-yarn.sh&lt;/code&gt;&lt;br&gt;开启历史服务器，以便在Web中查看任务运行情况&lt;code&gt;./sbin/mr-jobhistory-daemon.sh start historyserver&lt;/code&gt;&lt;br&gt;开启后通过jps查看，可以看到多了NodeManager和ResourceManager两个后台进程。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;浏览器打开&lt;code&gt;localhost:8088&lt;/code&gt;可以查看任务运行情况，跑一下WordCount程序。程序执行过程中可以实时查看运行进度，跑完后点击History可以查看作业详细信息。&lt;br&gt;&lt;img src=&quot;/img/hadoop/web_1.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;/img/hadoop/web_2.png&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;li&gt;关闭脚本：&lt;br&gt;&lt;code&gt;./sbin/mr-jobhistory-daemon.sh stop historyserver&lt;/code&gt;&lt;br&gt;&lt;code&gt;./sbin/stop-yarn.sh&lt;/code&gt;&lt;br&gt;&lt;code&gt;./sbin/stop-dfs.sh&lt;/code&gt;&lt;blockquote&gt;
&lt;p&gt;如果只启动YARN，那么只可以在Web查看任务运行状态，而不能查看历史信息。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;今天给大家带来Hadoop下的第一个Hello World程序。&lt;br&gt;
    
    </summary>
    
      <category term="Software" scheme="http://songbinbin.me/categories/Software/"/>
    
    
      <category term="Hadoop" scheme="http://songbinbin.me/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>在Ubuntu下布置Hadoop单机伪分布式</title>
    <link href="http://songbinbin.me/2016/10/21/Hadoop-install/"/>
    <id>http://songbinbin.me/2016/10/21/Hadoop-install/</id>
    <published>2016-10-21T12:01:37.000Z</published>
    <updated>2016-11-07T09:43:09.209Z</updated>
    
    <content type="html">&lt;p&gt;为了职业发展，开始试水大数据，今天带来一篇在Ubuntu14.04下布置Hadoop-2.7.1版本单机伪分布式的教程。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;1-安装和配置JDK&quot;&gt;&lt;a href=&quot;#1-安装和配置JDK&quot; class=&quot;headerlink&quot; title=&quot;1. 安装和配置JDK&quot;&gt;&lt;/a&gt;1. 安装和配置JDK&lt;/h1&gt;&lt;p&gt;在官网上下载JDK，解压并拷贝到&lt;code&gt;/usr/local&lt;/code&gt;下。接下来配置环境变量，打开&lt;code&gt;/etc/profile&lt;/code&gt;并添加&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;JAVA_HOME=/usr/&lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt;/jdk1.&lt;span class=&quot;number&quot;&gt;8.0&lt;/span&gt;_101&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;PATH=&lt;span class=&quot;variable&quot;&gt;$JAVA_HOME&lt;/span&gt;/bin:&lt;span class=&quot;variable&quot;&gt;$PATH&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;CLASSPATH=.:&lt;span class=&quot;variable&quot;&gt;$JAVA_HOME&lt;/span&gt;/lib/dt.jar:&lt;span class=&quot;variable&quot;&gt;$JAVA_HOME&lt;/span&gt;/lib/tools.jar&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; JAVA_HOME PATH CLASSPATH&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;保存退出，一般执行&lt;code&gt;source /etc/profile&lt;/code&gt;可以使配置立即生效，有时也需要重新打开terminal或者重启。&lt;br&gt;执行&lt;code&gt;java -version&lt;/code&gt;和&lt;code&gt;$JAVA_HOME/bin/java -version&lt;/code&gt;查看是否都会打印版本信息。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/hadoop/java_version.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;2-下载安装Hadoop&quot;&gt;&lt;a href=&quot;#2-下载安装Hadoop&quot; class=&quot;headerlink&quot; title=&quot;2. 下载安装Hadoop&quot;&gt;&lt;/a&gt;2. 下载安装Hadoop&lt;/h1&gt;&lt;p&gt;从Apache Hadoop官网下载一个稳定的发布包，这里我们下载2.7.1版本，解压到本地文件系统中，推荐就放在用户主目录下。&lt;/p&gt;
&lt;h1 id=&quot;3-配置SSH&quot;&gt;&lt;a href=&quot;#3-配置SSH&quot; class=&quot;headerlink&quot; title=&quot;3. 配置SSH&quot;&gt;&lt;/a&gt;3. 配置SSH&lt;/h1&gt;&lt;p&gt;为了保证在远程管理Hadoop节点以及Hadoop节点间用户共享访问时的安全性，Hadoop系统需要配置和使用SSH（安全外壳协议）。这里配置SSH的主要工作是创建一个认证文件，使得用户以public key方式登陆，而不用手工输密码。配置的基本步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;生成密钥对，执行命令&lt;code&gt;ssh-keygen -t rsa&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;一直按&lt;code&gt;Enter&lt;/code&gt;键，默认生成的密钥对保存在&lt;code&gt;.ssh/id_rsa&lt;/code&gt;文件中&lt;/li&gt;
&lt;li&gt;进入&lt;code&gt;.ssh&lt;/code&gt;目录，执行命令&lt;code&gt;cp id_rsa.pub authorized_keys&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;执行命令&lt;code&gt;ssh localhost&lt;/code&gt;即可测试是否登陆。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&quot;4-配置Hadoop环境&quot;&gt;&lt;a href=&quot;#4-配置Hadoop环境&quot; class=&quot;headerlink&quot; title=&quot;4. 配置Hadoop环境&quot;&gt;&lt;/a&gt;4. 配置Hadoop环境&lt;/h1&gt;&lt;p&gt;Hadoop-2.7.1版本需要配置三个文件，位于Hadoop下&lt;code&gt;etc/hadoop&lt;/code&gt;文件中，分别是Hadoop环境变量设置文件&lt;code&gt;hadoop-env.sh&lt;/code&gt;，全局配置文件&lt;code&gt;core-site.xml&lt;/code&gt;，HDFS配置文件&lt;code&gt;hdfs-site.xml&lt;/code&gt;。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;hadoop-env.sh&lt;/code&gt;配置文件，修改JAVA_HOME变量为jdk所在路径。&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; JAVA_HOME=/usr/&lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt;/jdk1.&lt;span class=&quot;number&quot;&gt;8.0&lt;/span&gt;_101&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;core-site.xml&lt;/code&gt;配置文件，在configuration中添加内容。其中，hadoop.tmp.dir的value为hadoop所在的路径。&lt;/p&gt;
&lt;figure class=&quot;highlight xml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;configuration&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;hadoop.tmp.dir&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;file:/home/tyrion/hadoop-2.7.1&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;description&lt;/span&gt;&amp;gt;&lt;/span&gt;Abase for other temporary directories.&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;description&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;fs.defaultFS&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;hdfs://localhost:9000&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;configuration&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;hdfs-site.xml&lt;/code&gt;配置文件，同样在configuration中添加内容。namenode和datanode的路径同样改为自己Hadoop所在的路径下。&lt;/p&gt;
&lt;figure class=&quot;highlight xml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;configuration&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;dfs.replication&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;1&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;dfs.namenode.name.dir&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;file:/home/tyrion/hadoop-2.7.1/tmp/dfs/name&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;dfs.datanode.data.dir&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;title&quot;&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;file:/home/tyrion/hadoop-2.7.1/tmp/dfs/data&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;title&quot;&gt;configuration&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;伪分布式虽然只需要配置fs.defaultFS和dfs.replicaton就可以运行，不过若没有配置hadoop.tmp.dir参数，则默认使用的临时目录为/tmp/hadoop/username，而这个目录重启时可能被系统清理掉，导致必须重新执行format才行。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&quot;5-Hadoop的运行&quot;&gt;&lt;a href=&quot;#5-Hadoop的运行&quot; class=&quot;headerlink&quot; title=&quot;5. Hadoop的运行&quot;&gt;&lt;/a&gt;5. Hadoop的运行&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;为方便在terminal中直接使用&lt;code&gt;hadoop&lt;/code&gt;命令，在自己用户主目录下的&lt;code&gt;.bashrc&lt;/code&gt;中添加环境变量。&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; PATH=/home/tyrion/hadoop-&lt;span class=&quot;number&quot;&gt;2.7&lt;/span&gt;.&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;/bin:&lt;span class=&quot;variable&quot;&gt;$PATH&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt; 同样使用命令&lt;code&gt;source .bashrc&lt;/code&gt;使其立即生效，执行&lt;code&gt;hadoop version&lt;/code&gt;查看是否显示Hadoop版本信息。&lt;br&gt;&lt;img src=&quot;/img/hadoop/hadoop_version.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;在初次安装和使用Hadoop之前，需要格式化分布式文件系统HDFS，使用命令&lt;code&gt;hadoop namenode -format&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在Hadoop目录下执行命令&lt;code&gt;./sbin/start-dfs.sh&lt;/code&gt;启动Hadoop，启动之后使用&lt;code&gt;jps&lt;/code&gt;命令查看启动的进程。&lt;br&gt;&lt;img src=&quot;/img/hadoop/start_dfs.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;执行命令&lt;code&gt;./sbin/stop-dfs.sh&lt;/code&gt;停止Hadoop守护进程。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;格式化命令只能在初次安装时使用，如果再次使用的话，namenode之前保存的datanode的信息会丢失，导致DataNode这个节点不会启动。解决办法是修改datanode文件夹中current下的VERSION文件，将其中的clusterID修改为namenode中对应的clusterID。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;至此便大功告成啦，接下来我会实现一些简单的MapReduce基础算法程序，加油咯！&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;为了职业发展，开始试水大数据，今天带来一篇在Ubuntu14.04下布置Hadoop-2.7.1版本单机伪分布式的教程。&lt;br&gt;
    
    </summary>
    
      <category term="Software" scheme="http://songbinbin.me/categories/Software/"/>
    
    
      <category term="Hadoop" scheme="http://songbinbin.me/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>305羽毛球排位赛</title>
    <link href="http://songbinbin.me/2016/05/03/badminton/"/>
    <id>http://songbinbin.me/2016/05/03/badminton/</id>
    <published>2016-05-03T00:35:38.000Z</published>
    <updated>2016-11-07T09:44:11.209Z</updated>
    
    <content type="html">&lt;p&gt;恭喜QQ糖再次战胜abl，后者继续巩固其实验室倒数第一的位置。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/2u9oVNMtBMc&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;恭喜QQ糖再次战胜abl，后者继续巩固其实验室倒数第一的位置。&lt;br&gt;
    
    </summary>
    
      <category term="Life" scheme="http://songbinbin.me/categories/Life/"/>
    
    
      <category term="QQ" scheme="http://songbinbin.me/tags/QQ/"/>
    
      <category term="badminton" scheme="http://songbinbin.me/tags/badminton/"/>
    
      <category term="video" scheme="http://songbinbin.me/tags/video/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://songbinbin.me/2016/02/27/Hello-World/"/>
    <id>http://songbinbin.me/2016/02/27/Hello-World/</id>
    <published>2016-02-27T01:57:15.000Z</published>
    <updated>2016-11-07T09:44:02.045Z</updated>
    
    <content type="html">&lt;p&gt;买下这个域名也有很久了，间间断断地搭起了这个博客，第一篇也就随便聊聊吧。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;照例，先给大家说声“Hello World!”。&lt;br&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;Hello World!&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;平时的生活还是略微单调，忙里偷闲打打炉石、看看NBA、和女朋友一起看看电影、打打羽毛球。没事会憧憬一下工作后财务自由的日子，也会为未来的不确定性偶尔担忧，但希望付出或多或少都会有收获。&lt;/p&gt;
&lt;p&gt;学习上，在南京大学这样一个理工科学校学着电子专业，研究生继续学电子却做着深度学习方面的研究，我也是够折腾的。好在实验室宽松的科研环境，能够让我专心做自己喜欢的方向；得益于迅捷的网络资源，也不会偏离现在的主流方向，很庆幸赶上了一个好时代。&lt;/p&gt;
&lt;p&gt;最后，本命年该干嘛干嘛，人生不留遗憾咯。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;买下这个域名也有很久了，间间断断地搭起了这个博客，第一篇也就随便聊聊吧。&lt;br&gt;
    
    </summary>
    
      <category term="Life" scheme="http://songbinbin.me/categories/Life/"/>
    
    
      <category term="diary" scheme="http://songbinbin.me/tags/diary/"/>
    
  </entry>
  
</feed>
